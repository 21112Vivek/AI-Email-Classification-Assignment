{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da5cf42-06c8-42bc-986e-877ce1e36a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\vivek\\bert_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1\n",
      "Epoch 1\n",
      "Avg Training Loss: 1.1168\n",
      "Validation Accuracy: 35.48%\n",
      "Epoch 2\n",
      "Avg Training Loss: 1.0095\n",
      "Validation Accuracy: 64.52%\n",
      "Epoch 3\n",
      "Avg Training Loss: 0.7703\n",
      "Validation Accuracy: 51.61%\n",
      "Epoch 4\n",
      "Avg Training Loss: 0.5269\n",
      "Validation Accuracy: 70.97%\n",
      "Epoch 5\n",
      "Avg Training Loss: 0.3529\n",
      "Validation Accuracy: 64.52%\n",
      "\n",
      "Training Fold 2\n",
      "Epoch 1\n",
      "Avg Training Loss: 0.5834\n",
      "Validation Accuracy: 96.77%\n",
      "Epoch 2\n",
      "Avg Training Loss: 0.4318\n",
      "Validation Accuracy: 96.77%\n",
      "Epoch 3\n",
      "Avg Training Loss: 0.2952\n",
      "Validation Accuracy: 90.32%\n",
      "Epoch 4\n",
      "Avg Training Loss: 0.2516\n",
      "Validation Accuracy: 83.87%\n",
      "Early stopping...\n",
      "\n",
      "Training Fold 3\n",
      "Epoch 1\n",
      "Avg Training Loss: 0.2691\n",
      "Validation Accuracy: 100.00%\n",
      "Epoch 2\n",
      "Avg Training Loss: 0.2286\n",
      "Validation Accuracy: 96.77%\n",
      "Epoch 3\n",
      "Avg Training Loss: 0.0607\n",
      "Validation Accuracy: 93.55%\n",
      "Epoch 4\n",
      "Avg Training Loss: 0.1928\n",
      "Validation Accuracy: 96.77%\n",
      "Early stopping...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.amp import GradScaler, autocast  \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set device for CUDA (GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', num_labels=3\n",
    ").to(device)\n",
    "\n",
    "\n",
    "input_texts = [\n",
    "    \"How to inquire about Product A?\", \"Tell me more about Product B.\", \"I need information on Product C.\",\n",
    "    \"Product A is interesting.\", \"How do I get Product B?\", \"I want to know more about Product C.\",\n",
    "    \"Inquiry about Product A please.\", \"Can you provide Product B details?\", \"Information on Product C?\",\n",
    "    \"Product A details requested.\"\n",
    "]\n",
    "\n",
    "# Additional Dataset for Better Learning\n",
    "additional_texts = [\n",
    "    \"Can you share specifications of Product A?\", \"I’m interested in Product B, tell me more.\", \"Give me insights on Product C.\",\n",
    "    \"How much does Product A cost?\", \"What’s the price of Product B?\", \"Tell me the pricing details for Product C.\",\n",
    "    \"Do you have any promotions on Product A?\", \"Any discounts available for Product B?\", \"Can I get a special deal on Product C?\",\n",
    "    \"Where can I buy Product A?\", \"Is Product B available in my region?\", \"How do I place an order for Product C?\",\n",
    "    \"Can you compare Product A and C?\", \"Which is better: Product B or C?\", \"What are the unique benefits of Product A?\",\n",
    "    \"Explain the advantages of using Product B.\", \"What are the key features of Product C?\", \"Do you offer free trials for Product A?\",\n",
    "    \"How can I request a demo of Product B?\", \"Is there a demo version of Product C?\", \"Can I return Product A if I don’t like it?\",\n",
    "    \"What’s the refund policy for Product B?\", \"Is there a return policy for Product C?\", \"Does Product A come with a warranty?\",\n",
    "    \"What’s the warranty coverage for Product B?\", \"How long is Product C’s warranty period?\", \"How do I install Product A?\",\n",
    "    \"Can you guide me through setting up Product B?\", \"What’s the installation process for Product C?\",\n",
    "    \"Where can I find the user manual for Product A?\", \"Can you send me the instruction guide for Product B?\", \"Is there a setup guide for Product C?\",\n",
    "    \"How do I update Product A?\", \"Are there any software updates for Product B?\", \"How frequently does Product C receive updates?\",\n",
    "    \"What customer support options are available for Product A?\", \"How do I contact technical support for Product B?\", \"Where can I get help with Product C?\",\n",
    "    \"What are the payment options for Product A?\", \"Can I pay in installments for Product B?\", \"Is there a subscription plan for Product C?\",\n",
    "    \"How secure is Product A?\", \"Does Product B have data protection features?\", \"What security measures does Product C include?\",\n",
    "    \"Is Product A compatible with other platforms?\", \"Can I integrate Product B with my existing tools?\", \"Does Product C work with third-party apps?\",\n",
    "    \"How do I deactivate my Product A account?\", \"Can I pause my subscription to Product B?\", \"How do I cancel my Product C service?\",\n",
    "    \"How to inquire about Product A?\", \"Tell me more about Product B.\", \"I need information on Product C.\",\n",
    "    \"Product A is interesting.\", \"How do I get Product B?\", \"I want to know more about Product C.\",\n",
    "    \"Inquiry about Product A please.\", \"Can you provide Product B details?\", \"Information on Product C?\",\n",
    "    \"Product A details requested.\", \"Can I learn more about Product A?\", \"Details about Product B are required.\",\n",
    "    \"Help me with Product C.\", \"What's the best way to contact for Product A?\", \"How can I receive more information on Product B?\",\n",
    "    \"I need some information on Product C right now.\", \"Inquire about Product A\", \"Provide me Product B details.\",\n",
    "    \"How do I purchase Product A?\", \"I want to ask about Product B's features.\", \"Tell me about Product C features.\",\n",
    "    \"Is there any special offer for Product A?\", \"Product B is the best option, right?\", \"I want to compare Product A with Product B.\",\n",
    "    \"How do I contact customer support for Product C?\", \"Give me details of Product A specs.\", \"What makes Product B special?\",\n",
    "    \"Where can I find reviews for Product C?\", \"Product A and B seem similar, what’s the difference?\", \"Can I get a discount on Product C?\",\n",
    "    \"How do I subscribe to Product A updates?\", \"Send me the brochure for Product B.\", \"Explain the warranty policy for Product C.\"\n",
    "]\n",
    "input_texts.extend(additional_texts)\n",
    "labels = [0, 1, 2] * (len(input_texts) // 3)  # Cycle labels evenly\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "encoded_data = tokenize_data(input_texts)\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_mask = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Check tensor sizes\n",
    "assert input_ids.size(0) == attention_mask.size(0) == labels.size(0), \"Mismatch in tensor sizes!\"\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize optimizer & scheduler with weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    scaler = GradScaler()  \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast('cuda'):  \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, preds)\n",
    "\n",
    "# Cross-validation\n",
    "def cross_validate():\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(input_ids, labels)):\n",
    "        print(f\"\\nTraining Fold {fold + 1}\")\n",
    "\n",
    "        train_loader = DataLoader(torch.utils.data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(torch.utils.data.Subset(dataset, val_idx), batch_size=batch_size)\n",
    "\n",
    "        best_fold_val_accuracy = 0\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(5):\n",
    "            print(f\"Epoch {epoch + 1}\")\n",
    "            avg_train_loss = train(model, train_loader, optimizer)\n",
    "            print(f\"Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            val_accuracy = validate(model, val_loader)\n",
    "            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "            if val_accuracy > best_fold_val_accuracy:\n",
    "                best_fold_val_accuracy = val_accuracy\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= 3:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "            scheduler.step(val_accuracy)\n",
    "\n",
    "cross_validate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b71dfb-ee60-4186-b904-f4d8d716561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 1.2560291290283203\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 1.058111548423767\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 1.001037359237671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Use AdamW optimizer for fine-tuning\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch + 1}: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9309c5-13d8-4f0b-9baf-b45b4fb302c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation after training\n",
    "model.eval()  # Set model to evaluation mode\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted class\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b976e9-bdd9-4647-9a52-3a92e8ad981d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert_env)",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
